{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granularity investigation\n",
    "\n",
    "In this notebook, we run our solver on old data with different granularity.\n",
    "\n",
    "Please check `transplants_investigation.ipynb` before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from typing import List\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../..\")\n",
    "\n",
    "from local_testing_utilities.notebook_utils.pairing_data import parse_pairing_data\n",
    "from local_testing_utilities.notebook_utils.survival_data import parse_survival_data\n",
    "from txmatching.utils.blood_groups import BloodGroup\n",
    "from txmatching.utils.country_enum import Country\n",
    "from txmatching.patients.patient import Donor, Recipient\n",
    "from txmatching.patients.patient_parameters import PatientParameters\n",
    "from tests.test_utilities.hla_preparation_utils import (create_antibodies, create_antibody,\n",
    "                                                        create_hla_typing)\n",
    "from tests.test_utilities.prepare_app_for_tests import DbTests\n",
    "from txmatching.patients.patient import TxmEvent\n",
    "from txmatching.utils.enums import TxmEventState\n",
    "from txmatching.configuration.configuration import Configuration\n",
    "from txmatching.solve_service.solve_from_configuration import solve_from_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_patients = parse_pairing_data('data/KDP-processed', 'data/patients_list_recipientID.csv', remove_single_donors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_survival = parse_survival_data('data/LD_kidney_survival.csv')\n",
    "df_survival_summary = pd.read_pickle('data/survival_summary.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients_with_recipient_id = pd.read_csv('data/patients_list_recipientID.csv')\n",
    "df_transplanted_donors = pd.read_excel('data/transplanted_donors.xlsx', index_col=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’â€â™‚ï¸ Run solver on data with various granularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To all patients records, we join transplants if found for both donors and recipients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_patients_with_survival = df_all_patients\\\n",
    "    .join(df_survival_summary.set_index('RecipientID').add_suffix('_recip_surv'), on='recipient_id')\\\n",
    "    .join(df_transplanted_donors.set_index('donor_name')['target_recipient_id'], on='donor_name')\\\n",
    "    .join(df_survival_summary.set_index('RecipientID').add_suffix('_donor_surv'), on='target_recipient_id')\\\n",
    "    .assign(recip_has_transplant=lambda df: df.delay_recip_surv.notnull())\\\n",
    "    .assign(donor_has_transplant=lambda df: df.delay_donor_surv.notnull())\n",
    "# df_all_patients_with_survival.head()\n",
    "# df_all_patients_with_survival[lambda df: df.donor_has_transplant]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define function that returns patients that would be in txm event with different granularity.\n",
    "\n",
    "For given granularity, each event has patients from the originla event plus patients from $granularity - 1$ previous events that have been transplanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recipients_for_granularity(txm_event, granularity):\n",
    "    df_patients_for_granularity = df_all_patients_with_survival.loc[\n",
    "        lambda df:\n",
    "        (df.txm_event == txm_event) | \n",
    "        ((df.txm_event > txm_event - granularity) & (df.txm_event < txm_event) & df.recip_has_transplant)\n",
    "    ]\n",
    "    df_patients_for_granularity = df_patients_for_granularity.drop_duplicates(subset=['recipient_id'], keep='last')#.reset_index()#.set_index('recipient_id')\n",
    "    df_patients_for_granularity = df_patients_for_granularity.loc[lambda df: df.recipient_id.notnull()]\n",
    "    return df_patients_for_granularity\n",
    "\n",
    "#get_recipients_for_granularity(txm_event=26, granularity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_donors_for_granularity(txm_event, granularity):\n",
    "    df_patients_for_granularity = df_all_patients_with_survival.loc[\n",
    "        lambda df:\n",
    "        (df.txm_event == txm_event) | \n",
    "        ((df.txm_event > txm_event - granularity) & (df.txm_event < txm_event) & df.donor_has_transplant)\n",
    "    ]\n",
    "    df_patients_for_granularity = df_patients_for_granularity.drop_duplicates(subset=['donor_name'], keep='last')#.reset_index()#.set_index('donor_name')\n",
    "    return df_patients_for_granularity\n",
    "\n",
    "#get_donors_for_granularity(txm_event=26, granularity=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we run the solver, we make some config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize db\n",
    "test = DbTests()\n",
    "test.setUp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test.tearDown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "logger.setLevel('WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the solver. We define several functions for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_recipient(db_id, row):\n",
    "    recipient_typization = row.recipient_typization\n",
    "    recipient_antibodies = row.recipient_luminex_2\n",
    "    \n",
    "    recipient_blood_group = row.recipient_blood_group\n",
    "    recipient_acceptable_blood = row.recipient_acceptable_blood\n",
    "    \n",
    "    db_id = row.recipient_id\n",
    "    \n",
    "    if recipient_typization == '':\n",
    "        print(f\"Problem with recipient empty typization {db_id}\")\n",
    "        return None\n",
    "    \n",
    "    recipient_typing = recipient_typization.split(\" \")\n",
    "    recipient_antibodies = recipient_antibodies.split(\" \") if recipient_antibodies != '' else []\n",
    "    recipient_acceptable_blood = recipient_acceptable_blood.split(\" \") if recipient_acceptable_blood != '' else []\n",
    "    \n",
    "    recipient = Recipient(\n",
    "        db_id=db_id,\n",
    "        acceptable_blood_groups=[BloodGroup(group) for group in recipient_acceptable_blood],\n",
    "        related_donor_db_id=db_id,\n",
    "        medical_id=f'recipient_{db_id}',\n",
    "        parameters=PatientParameters(\n",
    "            blood_group=BloodGroup(recipient_blood_group),\n",
    "            country_code=Country.CZE,\n",
    "            hla_typing=create_hla_typing(recipient_typing)\n",
    "        ),\n",
    "        hla_antibodies=create_antibodies([create_antibody(raw_code, 2000, 2000) for raw_code in recipient_antibodies])\n",
    "    )\n",
    "    \n",
    "    return recipient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_to_donor(db_id, row):\n",
    "    donor_typization = row.donor_typization\n",
    "    donor_blood_group = row.donor_blood_group\n",
    "    donor_is_single = math.isnan(row.recipient_id)\n",
    "    \n",
    "    if not donor_is_single:\n",
    "        db_id = row.recipient_id\n",
    "    \n",
    "    if donor_typization == '':\n",
    "        print(f\"Problem with donor empty typization {db_id}\")\n",
    "        return None\n",
    "    \n",
    "    donor_typing = donor_typization.split(\" \")\n",
    "    \n",
    "    donor = Donor(\n",
    "        db_id=db_id,\n",
    "        medical_id=f'donor_{db_id}',\n",
    "        related_recipient_db_id=db_id if not donor_is_single else None,\n",
    "        parameters=PatientParameters(\n",
    "            blood_group=BloodGroup(donor_blood_group),\n",
    "            country_code=Country.CZE,\n",
    "            hla_typing=create_hla_typing(\n",
    "                donor_typing\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return donor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_for_patients(df_donors, df_recipients, forbidden_donor_ids = set(), forbidden_recipient_ids = set()):\n",
    "    donors = []\n",
    "    recipients = []\n",
    "    \n",
    "    for index, row in df_donors.iterrows():\n",
    "        maybe_donor = row_to_donor(int(index), row)\n",
    "        if maybe_donor is None or maybe_donor.medical_id in forbidden_donor_ids:\n",
    "            continue\n",
    "        \n",
    "        donors.append(maybe_donor)\n",
    "    \n",
    "    for index, row in df_recipients.iterrows():\n",
    "        maybe_recipient = row_to_recipient(int(index), row)\n",
    "        if maybe_recipient is None or maybe_recipient.medical_id in forbidden_recipient_ids:\n",
    "            continue\n",
    "        \n",
    "        recipients.append(maybe_recipient)\n",
    "\n",
    "    txm_event = TxmEvent(1, 'sample_event', None, TxmEventState.OPEN, donors, recipients)\n",
    "    configuration = Configuration(\n",
    "        max_number_of_matchings=1,\n",
    "        max_cycle_length=50,\n",
    "        max_sequence_length=50,\n",
    "        # solver_constructor_name='AllSolutionsSolver'\n",
    "    )\n",
    "    pairing_result = solve_from_configuration(configuration, txm_event=txm_event)\n",
    "\n",
    "    matchings_count = len(pairing_result.calculated_matchings_list)\n",
    "    \n",
    "    transplanted_donors_ids = set()\n",
    "    transplanted_recipients_ids = set()\n",
    "    \n",
    "    if matchings_count > 0:\n",
    "        matching = pairing_result.calculated_matchings_list[0]\n",
    "        matching_pairs_count = len(matching.get_donor_recipient_pairs())\n",
    "        #print(pairing_result.score_matrix)\n",
    "        for pair in matching.get_donor_recipient_pairs():\n",
    "            #print(f\"{pair.donor.medical_id} -> {pair.recipient.medical_id}\")\n",
    "            transplanted_donors_ids.add(pair.donor.medical_id)\n",
    "            transplanted_recipients_ids.add(pair.recipient.medical_id)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        matching_pairs_count = 0\n",
    "    \n",
    "    return matching_pairs_count, len(donors), len(recipients), transplanted_donors_ids, transplanted_recipients_ids\n",
    "\n",
    "# df_donors = get_donors_for_granularity(txm_event=24, granularity=4)\n",
    "# df_recipients = get_recipients_for_granularity(txm_event=24, granularity=4)\n",
    "# display(df_donors)\n",
    "# df_recipients[df_recipients.columns[:20]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for granularity in range(1, 5):\n",
    "    for shift in range(granularity):\n",
    "        forbidden_donor_ids = set()\n",
    "        forbidden_recipient_ids = set()\n",
    "        for txm_event in range(10, 31):\n",
    "            if txm_event % granularity != shift:\n",
    "                continue\n",
    "            \n",
    "            df_donors = get_donors_for_granularity(txm_event=txm_event, granularity=granularity)\n",
    "            df_recipients = get_recipients_for_granularity(txm_event=txm_event, granularity=granularity)\n",
    "\n",
    "            donors_count = len(df_donors.index)\n",
    "            recipients_count = len(df_recipients.index)\n",
    "\n",
    "            print(f\"Computing matching for txm_event {txm_event} and granularity {granularity} ({donors_count} donors, {recipients_count} recipients, forbidden={len(forbidden_donor_ids)}/{len(forbidden_recipient_ids)})\", end=\" \")\n",
    "            start = time.time()\n",
    "            matching_pairs_count, valid_donors, valid_recipients, transplanted_donors_ids, transplanted_recipients_ids = \\\n",
    "                compute_for_patients(df_donors, df_recipients, forbidden_donor_ids, forbidden_recipient_ids)\n",
    "            elapsed_time = time.time() - start\n",
    "            print(f\"-> {matching_pairs_count} transplants found ({elapsed_time:.2f} seconds)\")\n",
    "\n",
    "            d.append({\n",
    "                'txm_event': txm_event,\n",
    "                'granularity': granularity,\n",
    "                'donors_count': donors_count,\n",
    "                'recipients_count': recipients_count,\n",
    "                'valid_donors': valid_donors,\n",
    "                'valid_recipients': valid_recipients,\n",
    "                'forbidden_count': len(forbidden_donor_ids),\n",
    "                'matching_pairs_count': matching_pairs_count,\n",
    "                'matching_pairs_count_normalized': matching_pairs_count / granularity,\n",
    "                'elapsed_time': elapsed_time\n",
    "            })\n",
    "            \n",
    "            forbidden_donor_ids.update(transplanted_donors_ids)\n",
    "            forbidden_recipient_ids.update(transplanted_recipients_ids)\n",
    "\n",
    "df_granularity_results = pd.DataFrame(d).sort_values(by=['txm_event'])\n",
    "df_granularity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_granularity_results.to_csv('data/granularity_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is computed now. Lets show some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values=['donors_count']).plot(ylabel='Donors count')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='recipients_count').plot(ylabel='Recipients count')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='valid_donors').plot(ylabel='Valid donors')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='forbidden_count').plot(ylabel='Forbidden count')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='matching_pairs_count').plot(ylabel='matching_pairs_count')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='matching_pairs_count_normalized').plot(ylabel='matching_pairs_count_normalized')\n",
    "df_granularity_results.pivot_table(index='txm_event', columns='granularity', values='elapsed_time').plot(ylabel='elapsed_time (s)')\n",
    "\n",
    "for granularity in range(1, 5):\n",
    "    df_granularity_results\\\n",
    "        .loc[lambda df: df.granularity == granularity]\\\n",
    "        .set_index('txm_event')\\\n",
    "        [['donors_count', 'recipients_count', 'matching_pairs_count']]\\\n",
    "        .plot(title=f'granularity = {granularity}')\n",
    "        #.assign(ratio=lambda df: df.matching_pairs_count / df.patients_count)\\\n",
    "        #.assign(diff=lambda df: - df.matching_pairs_count + df.patients_count)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare with original transplant count\n",
    "Now, lets suppose granularity = 1 and compare the computed results with original transplant count that we know using survival data.\n",
    "\n",
    "Note: The original transplant count was moved to `transplants_investigation`.\n",
    "\n",
    "Our algorthm found less transplants because it utilizes czech patients only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group by 1 year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_per_year = df_granularity_results\\\n",
    "    .assign(year=lambda df: (df.txm_event-10)//4)\\\n",
    "    .assign(event_in_year=lambda df: (df.txm_event-10)%4)\n",
    "df_results_per_year['granularity_1'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 1 else 0, axis=1)\n",
    "df_results_per_year['granularity_2_shift_0'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 2 and s.event_in_year in [0, 2] else 0, axis=1)\n",
    "df_results_per_year['granularity_2_shift_1'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 2 and s.event_in_year in [1, 3] else 0, axis=1)\n",
    "df_results_per_year = df_results_per_year.assign(granularity_2_mean=lambda df: (df.granularity_2_shift_0 + df.granularity_2_shift_1) / 2)\n",
    "df_results_per_year['granularity_4_shift_0'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 4 and s.event_in_year == 0 else 0, axis=1)\n",
    "df_results_per_year['granularity_4_shift_1'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 4 and s.event_in_year == 1 else 0, axis=1)\n",
    "df_results_per_year['granularity_4_shift_2'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 4 and s.event_in_year == 2 else 0, axis=1)\n",
    "df_results_per_year['granularity_4_shift_3'] = df_results_per_year.apply(lambda s: s.matching_pairs_count if s.granularity == 4 and s.event_in_year == 3 else 0, axis=1)\n",
    "df_results_per_year = df_results_per_year.assign(granularity_4_mean=lambda df: (df.granularity_4_shift_0 + df.granularity_4_shift_1 + df.granularity_4_shift_2 + df.granularity_4_shift_3) / 4)\n",
    "\n",
    "df_results_per_year = df_results_per_year.groupby(['year']).sum().reset_index()\n",
    "\n",
    "# Do not show data from txm events that are not complete\n",
    "df_results_per_year.loc[0, 'granularity_2_shift_0'] = None\n",
    "df_results_per_year.loc[0, 'granularity_4_shift_0'] = None\n",
    "df_results_per_year.loc[0, 'granularity_4_shift_1'] = None\n",
    "df_results_per_year.loc[0, 'granularity_4_shift_2'] = None\n",
    "\n",
    "df_results_per_year = df_results_per_year[[\n",
    "    'granularity_1',\n",
    "    'granularity_2_shift_0',\n",
    "    'granularity_2_shift_1',\n",
    "    'granularity_2_mean',\n",
    "    'granularity_4_shift_0',\n",
    "    'granularity_4_shift_1',\n",
    "    'granularity_4_shift_2',\n",
    "    'granularity_4_shift_3',\n",
    "    'granularity_4_mean'\n",
    "]]\n",
    "\n",
    "df_results_per_year.plot(\n",
    "    figsize=(15, 7), style=['b-','g-.','g--','g-','r-.','r--','r-o','r-.','r-'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall number of transplants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_per_year.loc[1:].sum().plot.barh()\n",
    "df_results_per_year.loc[1:].sum()[['granularity_1', 'granularity_2_mean', 'granularity_4_mean']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show dependency between patient count and found transplants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_patients(n):\n",
    "    return df_all_patients.loc[lambda df: df.recipient_id.notnull()].drop_duplicates(subset=['recipient_id'], keep='last').sample(n=n)\n",
    "# get_random_patients(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "\n",
    "for n in range(0, 40):\n",
    "    print(f\"Computing matching for ({n} patients)\", end=\" \")\n",
    "    \n",
    "    for i in range(20 if n < 10 else 5):\n",
    "        df_random_patients = get_random_patients(n)\n",
    "        \n",
    "        patients_count = len(df_random_patients.index)\n",
    "        \n",
    "        start = time.time()\n",
    "        matching_pairs_count, valid_donors, valid_recipients = compute_for_patients(df_random_patients, df_random_patients)\n",
    "        elapsed_time = time.time() - start\n",
    "        print(f\", {matching_pairs_count} ({elapsed_time:.2f}s)\", end=\" \")\n",
    "        \n",
    "        d.append({\n",
    "            'patients': patients_count,\n",
    "            'valid_donors': valid_donors,\n",
    "            'valid_recipients': valid_recipients,\n",
    "            'matching_pairs_count': matching_pairs_count,\n",
    "            'elapsed_time': elapsed_time\n",
    "        })\n",
    "    print()\n",
    "\n",
    "df_ratio_results = pd.DataFrame(d)\n",
    "df_ratio_results.to_csv('data/ratio_results.csv')\n",
    "# df_ratio_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio_results.groupby('patients').agg({'matching_pairs_count': ['mean', 'std']}).plot(xlim=0, ylim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio_results.assign(ratio=lambda df: df.matching_pairs_count/df.patients).groupby('patients').agg({'ratio': ['mean', 'std']}).plot(xlim=0, ylim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratio_results.groupby('patients').agg({'elapsed_time': ['mean', 'std']}).plot(xlim=0, ylim=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TXM new",
   "language": "python",
   "name": "txmatching"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
